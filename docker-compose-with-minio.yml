services:
  namenode:
    image: adbgonzalez/spark:3.5.7
    container_name: namenode
    hostname: namenode
    ports:
      - "10071:50070"
      - "9000:9000"
      - "9870:9870"
    networks:
      - cluster
    volumes:
      - namenode_data:/home/hadoop/namenode
      - ./start-dfs.sh:/start-dfs.sh
    command: /bin/bash /start-dfs.sh 


  datanode:
    image: adbgonzalez/spark:3.5.7
    container_name: datanode
    hostname: datanode
    networks:
      - cluster
    volumes:
      - datanode_data:/home/hadoop/datanode/
    command: hdfs datanode

  resourcemanager:
    image: adbgonzalez/spark:3.5.7
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"
    networks:
      - cluster
    command: yarn resourcemanager

  nodemanager:
    image: adbgonzalez/spark:3.5.7
    container_name: nodemanager
    hostname: nodemanager
    networks:
      - cluster
    command: yarn nodemanager

 # ========= Spark History Server =========
  spark-history:
    image: adbgonzalez/spark:3.5.7
    container_name: spark-history
    hostname: spark-history
    user: "1000:1000"
    depends_on:
      - namenode
      - resourcemanager
    ports:
      - "18080:18080"
    networks:
      - cluster
    environment:
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
      SPARK_ROLE: "history"
      SPARK_LOG_DIR: /opt/spark/logs
      SPARK_HISTORY_UI_PORT: "18080"
      SPARK_CONF_DIR: /opt/spark/conf
      # (opcional) ampliar classpath cos jars extra (non obrigatorio se só usas HDFS)
      SPARK_EXTRA_CLASSPATH: /opt/spark/jars-extra/*
    volumes:
      # ⚠️ non montes sobre /opt/spark/jars
      - ./jars:/opt/spark/jars-extra:ro
      - ./spark-conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./start-history.sh:/start-history.sh:ro
    command: ["bash","-lc","/start-history.sh"]

  notebook:
    image: adbgonzalez/spark-notebook:3.5.7
    container_name: spark-notebook
    hostname: spark-notebook
    user: "1000:1000"
    depends_on:
      - resourcemanager
      - datanode
    networks:
      - cluster
    ports:
      - "8888:8888"
    environment:
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
      PYSPARK_PYTHON: python3
      SPARK_CONF_DIR: /opt/spark/conf
      # para que PySpark vexa os JARs de MinIO automaticamente
      SPARK_EXTRA_CLASSPATH: /opt/spark/jars-extra/*
      SHELL: /bin/bash
    volumes:
      - ./work:/home/hadoop/work
      - ./jars:/opt/spark/jars-extra:ro
      - ./spark-conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    command:
      - bash
      - -lc
      - >
        jupyter lab --ip=0.0.0.0 --port=8888
        --NotebookApp.token=''
        --NotebookApp.password=''
        --no-browser

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9001:9001"   # consola
      - "9002:9000"   # API interna 9000 → publicada como 9002 no host
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - cluster

networks:
  cluster:
    driver: bridge

volumes:
  namenode_data:
  datanode_data:
  minio_data: