FROM ubuntu:20.04

ENV DEBIAN_FRONTEND=noninteractive
ENV HADOOP_VERSION=3.4.2
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV HADOOP_HOME=/usr/local/hadoop
ENV HADOOP_INSTALL=$HADOOP_HOME
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV HADOOP_YARN_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

# Instalar paquetes (sen recomendados) e limpar caches NA MESMA CAPA
RUN apt-get update \
 && apt-get install -y --no-install-recommends \
      openjdk-11-jdk-headless \
      ssh \
      rsync \
      wget \
      curl \
      net-tools \
      ca-certificates \
      python3 \
 && rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/*

# Crear usuario/grupo
RUN groupadd --gid 1000 hadoop \
 && useradd --uid 1000 --gid 1000 --create-home --shell /bin/bash hadoop

# Hadoop
WORKDIR /home/hadoop

# Descargar, extraer e borrar o tarball na MESMA capa
RUN set -eux; \
    url="https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}-lean.tar.gz"; \
    wget -O /tmp/hadoop.tar.gz "$url"; \
    tar -xzf /tmp/hadoop.tar.gz -C /usr/local/; \
    mv /usr/local/hadoop-${HADOOP_VERSION} ${HADOOP_HOME}; \
    rm -f /tmp/hadoop.tar.gz; \
    chown -R hadoop:hadoop ${HADOOP_HOME}

# Dirs de datos
RUN install -d -o hadoop -g hadoop /home/hadoop/namenode /home/hadoop/datanode

# Config
COPY conf/core-site.xml $HADOOP_HOME/etc/hadoop/
COPY conf/hdfs-site.xml $HADOOP_HOME/etc/hadoop/
COPY conf/mapred-site.xml $HADOOP_HOME/etc/hadoop/
COPY conf/yarn-site.xml $HADOOP_HOME/etc/hadoop/

USER hadoop

# SSH para pseudo-distribuído
RUN mkdir -p ~/.ssh \
 && ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa \
 && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys \
 && chmod 700 ~/.ssh \
 && chmod 600 ~/.ssh/authorized_keys

 # Spark 
ARG SPARK_VERSION=3.5.7
ARG SPARK_FILE=spark-3.5.7-bin-hadoop3.tgz
ARG SPARK_URL=https://downloads.apache.org/spark/spark-$SPARK_VERSION/$SPARK_FILE
ENV SPARK_HOME=/opt/spark
RUN wget -q $SPARK_URL   && \
    mkdir $SPARK_HOME && \
    tar -xzf $SPARK_FILE   -C $SPARK_HOME --strip-components 1 && \
    rm $SPARK_FILE

# variables de entorno 
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 
ENV SPARK_HOME=/opt/spark 
ENV SPARK_VERSION=3.5.0
ENV SPARK_CONF=${SPARK_HOME}/conf 
ENV SPARK_LOG_DIR=hdfs:///spark/logs 
ENV SPARK_HISTORY_UI_PORT=18080 
ENV SPARK_EVENTLOG_ENABLED=true 
ENV SPARK_HISTORY_FS_LOG_DIRECTORY=hdfs:///spark/logs/history 
ENV SPARK_EVENT_LOG_DIR=$SPARK_HISTORY_FS_LOG_DIRECTORY 
ENV SPARK_DAEMON_MEMORY=10g 
ENV SPARK_HISTORY_FS_CLEANER_ENABLED=true 
ENV SPARK_HISTORY_STORE_MAXDISKUSAGE=100g 
ENV SPARK_HISTORY_FS_CLEANER_INTERVAL=8h 
ENV SPARK_HISTORY_FS_CLEANER_MAXAGE=5d 
ENV SPARK_HISTORY_FS_UPDATE_INTERVAL=10s 
ENV SPARK_HISTORY_RETAINED_APPLICATIONS=100 
ENV SPARK_HISTORY_UI_MAXAPPLICATIONS=500
ENV PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH 
ENV HADOOP_INSTALL=$HADOOP_HOME
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV HADOOP_YARN_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativ
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

ENV SPARK_ROLE=master
# Exponemos el puerto de la interfaz web de Spark (8080)
EXPOSE 8080
EXPOSE 4040
EXPOSE 7077
EXPOSE 18080 
EXPOSE 8088 8042 50070 50075 50090 8020 9000 9870

RUN ln $HADOOP_CONF_DIR/workers $SPARK_CONF/ 

# Directorio de salida para el servidor de historial
RUN mkdir -p /opt/spark/logs/history
RUN chmod a+w /opt/spark/logs/history



# Copiamos los scripts de entrada a la imagen
COPY ./entrypoint.sh /

# Concedemos permisos de ejecución al script de entrada
RUN chmod +x /entrypoint.sh

# Directorio de trabajo
WORKDIR /opt/spark

# Comando por defecto al iniciar el contenedor (puedes cambiarlo según tus necesidades)
CMD ["/bin/bash", "-c", "/entrypoint.sh"]


#CMD /bin/bash -c yes