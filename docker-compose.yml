services:
  namenode:
    image: adbgonzalez/spark:test-lean
    container_name: namenode
    hostname: namenode
    ports:
      - "10071:50070"
      - "9000:9000"
      - "9870:9870"
    networks:
      - hadoop
    volumes:
      - namenode_data:/home/hadoop/namenode
      - ./start-dfs.sh:/start-dfs.sh
    command: /bin/bash /start-dfs.sh 


  datanode:
    image: adbgonzalez/spark:test-lean
    container_name: datanode
    hostname: datanode
    networks:
      - hadoop
    volumes:
      - datanode_data:/home/hadoop/datanode/
    command: hdfs datanode

  resourcemanager:
    image: adbgonzalez/spark:test-lean
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"
    networks:
      - hadoop
    command: yarn resourcemanager

  nodemanager:
    image: adbgonzalez/spark:test-lean
    container_name: nodemanager
    hostname: nodemanager
    networks:
      - hadoop
    command: yarn nodemanager

 # ========= Spark History Server =========
  spark-history:
    image: adbgonzalez/spark:test-lean
    container_name: spark-history
    hostname: spark-history
    depends_on:
      - namenode
      - resourcemanager
    ports:
      - "18080:18080"
    networks:
      - hadoop
    environment:
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
      SPARK_ROLE: "history"
      SPARK_LOG_DIR: /opt/spark/logs
      SPARK_HISTORY_UI_PORT: "18080"
    volumes:
      - ./start-history.sh:/start-history.sh:ro
    command: ["bash","-lc","/start-history.sh"]


  # ========= Cliente para probar =========
  spark-client:
    image: adbgonzalez/spark:test-lean
    container_name: spark-client
    hostname: spark-client
    user: hadoop
    depends_on:
      - resourcemanager
      - datanode
    environment:
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
      PYSPARK_PYTHON: python3
    working_dir: /opt/spark
    command: ["bash","-lc","sleep infinity"]
    networks:
      - hadoop
  notebook:
    image: adbgonzalez/spark-notebook:test-lean
    container_name: spark-notebook
    hostname: spark-notebook
    user: "1000:1000"
    depends_on:
      - resourcemanager
      - datanode
    networks:
      - hadoop
    ports:
      - "8888:8888"
    environment:
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
      PYSPARK_PYTHON: python3
      SPARK_CONF_DIR: /opt/spark/conf
    volumes:
      - ./work:/home/hadoop/work
    command: bash -lc "jupyter lab --ip=0.0.0.0 --port=8888 --NotebookApp.token='' --NotebookApp.password='' --no-browser"

networks:
  hadoop:
    driver: bridge

volumes:
  namenode_data:
  datanode_data: